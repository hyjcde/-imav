#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§†é¢‘ç‰‡æ®µåˆ†æå™¨
å¯¹çŸ­è§†é¢‘ç‰‡æ®µè¿›è¡Œé€å¸§è¯¦ç»†åˆ†æï¼Œå±•ç¤ºæ£€æµ‹ä¸è¯­ä¹‰ç†è§£è¿‡ç¨‹
"""

import cv2
import numpy as np
import os
import json
from datetime import datetime
from argparse import ArgumentParser
from typing import List, Dict, Any
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import time

from drone_optimized_detector import DroneOptimizedPersonFinder
from utils.clip_utils import load_clip, compute_clip_similarity


def create_analysis_overlay(frame: np.ndarray, detections: List[Dict], 
                          clip_scores: List[float], frame_num: int, 
                          query_text: str) -> np.ndarray:
    """åˆ›å»ºè¯¦ç»†åˆ†æè¦†ç›–å›¾"""
    overlay = frame.copy()
    h, w = frame.shape[:2]
    
    # æ”¹è¿›çš„æ ‡é¢˜ä¿¡æ¯ï¼ˆå³ä¸Šè§’ï¼ŒåŠé€æ˜ï¼‰
    title_bg_height = 140
    title_width = 500
    # å®Œå…¨ä¸é€æ˜çš„é»‘è‰²èƒŒæ™¯ï¼Œç¡®ä¿æ–‡å­—æ¸…æ™°
    cv2.rectangle(overlay, (w - title_width, 0), (w, title_bg_height), (0, 0, 0), -1)
    cv2.rectangle(overlay, (w - title_width, 0), (w, title_bg_height), (255, 255, 255), 2)
    
    # å¤„ç†ä¸­æ–‡æ˜¾ç¤ºï¼ˆç®€åŒ–ä¸ºè‹±æ–‡é¿å…ç¼–ç é—®é¢˜ï¼‰
    display_query = query_text
    if 'çº¢è‰²å¤´ç›”' in query_text:
        display_query = 'red helmet'
    elif 'é»„è‰²' in query_text:
        display_query = 'yellow vest'
    elif 'è“è‰²' in query_text:
        display_query = 'blue clothing'
    
    title_lines = [
        f"Frame {frame_num}",
        f"Query: {display_query}",
        f"Detected: {len(detections)} persons",
        f"Max CLIP: {max(clip_scores):.3f}" if clip_scores else "Max CLIP: N/A"
    ]
    
    for i, line in enumerate(title_lines):
        cv2.putText(overlay, line, (w - title_width + 15, 30 + i*30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    
    # å…ˆç»˜åˆ¶æ‰€æœ‰æ£€æµ‹æ¡†ï¼ˆç»†çº¿ï¼Œæµ…è‰²ï¼‰
    for i, det in enumerate(detections):
        x1, y1, x2, y2 = det['bbox']
        cv2.rectangle(overlay, (x1, y1), (x2, y2), (180, 180, 180), 2)
        cv2.putText(overlay, f"P{i+1}", (x1+2, max(y1-8, 20)), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (180, 180, 180), 2)
    
    # å†ç»˜åˆ¶é«˜CLIPåˆ†æ•°çš„æ£€æµ‹æ¡†ï¼ˆç²—çº¿ï¼Œé²œè‰³è‰²ï¼‰
    for i, (det, clip_score) in enumerate(zip(detections, clip_scores)):
        if clip_score < 0.25:  # é™ä½é˜ˆå€¼ï¼Œæ˜¾ç¤ºæ›´å¤šå€™é€‰
            continue
            
        x1, y1, x2, y2 = det['bbox']
        conf = det['confidence']
        
        # æ ¹æ®CLIPè¯­ä¹‰åˆ†æ•°é€‰æ‹©é¢œè‰²
        if clip_score > 0.55:
            color = (0, 0, 255)  # çº¢è‰² - é«˜åŒ¹é…
            thickness = 6
        elif clip_score > 0.35:
            color = (0, 255, 255)  # é»„è‰² - ä¸­åŒ¹é…
            thickness = 5
        else:
            color = (0, 150, 255)  # æ©™è‰² - ä½åŒ¹é…
            thickness = 4
        
        # ç»˜åˆ¶æ£€æµ‹æ¡†
        cv2.rectangle(overlay, (x1, y1), (x2, y2), color, thickness)
        
        # å¤§å­—ä½“æ¸…æ™°æ ‡ç­¾
        main_label = f"P{i+1} CLIP:{clip_score:.3f}"
        font_scale = 0.8
        thickness_text = 2
        
        # æ ‡ç­¾ä½ç½®ï¼ˆé¿å…é‡å ï¼Œä¼˜å…ˆä¸Šæ–¹ï¼‰
        label_size = cv2.getTextSize(main_label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness_text)[0]
        label_x = max(x1, 10)
        label_y = max(y1 - 35, 40)
        
        # ç™½è‰²èƒŒæ™¯ + é»‘è‰²è¾¹æ¡†ï¼Œç¡®ä¿æœ€ä½³å¯¹æ¯”åº¦
        cv2.rectangle(overlay, (label_x - 5, label_y - 25), 
                     (label_x + label_size[0] + 10, label_y + 8), (255, 255, 255), -1)
        cv2.rectangle(overlay, (label_x - 5, label_y - 25), 
                     (label_x + label_size[0] + 10, label_y + 8), (0, 0, 0), 2)
        
        # é»‘è‰²æ–‡å­—ï¼Œæœ€å¤§å¯¹æ¯”åº¦
        cv2.putText(overlay, main_label, (label_x, label_y),
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 0), thickness_text)
        
        # å¦‚æœæ˜¯æœ€é«˜åˆ†ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°
        if clip_scores and clip_score == max(clip_scores) and clip_score > 0.4:
            center_x, center_y = (x1+x2)//2, (y1+y2)//2
            cv2.circle(overlay, (center_x, center_y), 25, (0, 0, 255), 4)
            # TARGETæ ‡ç­¾ç”¨ç™½åº•é»‘å­—
            target_label = "TARGET"
            target_size = cv2.getTextSize(target_label, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 3)[0]
            cv2.rectangle(overlay, (center_x - target_size[0]//2 - 8, center_y + 30), 
                         (center_x + target_size[0]//2 + 8, center_y + 65), (255, 255, 255), -1)
            cv2.rectangle(overlay, (center_x - target_size[0]//2 - 8, center_y + 30), 
                         (center_x + target_size[0]//2 + 8, center_y + 65), (0, 0, 0), 2)
            cv2.putText(overlay, target_label, (center_x - target_size[0]//2, center_y + 55),
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 3)
    
    return overlay


def create_crop_analysis_grid(detections: List[Dict], clip_scores: List[float], 
                            query_text: str, frame_num: int, output_dir: str):
    """åˆ›å»ºäººç‰©è£å‰ªåˆ†æç½‘æ ¼å›¾"""
    if not detections:
        return
    
    # è®¡ç®—ç½‘æ ¼å¸ƒå±€
    n_persons = len(detections)
    cols = min(4, n_persons)
    rows = (n_persons + cols - 1) // cols
    
    fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*4))
    if rows == 1 and cols == 1:
        axes = [axes]
    elif rows == 1:
        axes = axes
    else:
        axes = axes.flatten()
    
    fig.suptitle(f'Frame {frame_num} - Person Crops Analysis\nQuery: "{query_text}"', 
                fontsize=14, fontweight='bold')
    
    for i, (det, clip_score) in enumerate(zip(detections, clip_scores)):
        if i >= len(axes):
            break
            
        crop = det.get('person_crop')
        if crop is not None and crop.size > 0:
            # è½¬æ¢BGRåˆ°RGB
            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)
            axes[i].imshow(crop_rgb)
            
            # æ ‡é¢˜ä¿¡æ¯
            conf = det['confidence']
            size = det['size']
            title = f'Person {i+1}\nCLIP: {clip_score:.3f}\nConf: {conf:.3f}\nSize: {size[0]}Ã—{size[1]}'
            
            # æ ¹æ®CLIPåˆ†æ•°è®¾ç½®æ ‡é¢˜é¢œè‰²
            if clip_score > 0.6:
                title_color = 'red'
            elif clip_score > 0.4:
                title_color = 'orange'
            else:
                title_color = 'green'
                
            axes[i].set_title(title, fontsize=10, color=title_color, fontweight='bold')
        else:
            axes[i].text(0.5, 0.5, 'No Crop', ha='center', va='center', transform=axes[i].transAxes)
            axes[i].set_title(f'Person {i+1}\nNo Crop Available', fontsize=10)
        
        axes[i].axis('off')
    
    # éšè—æœªä½¿ç”¨çš„å­å›¾
    for i in range(n_persons, len(axes)):
        axes[i].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, f'frame_{frame_num}_crops_analysis.png'), 
               dpi=150, bbox_inches='tight')
    plt.close()


def analyze_video_segment(video_path: str, query_text: str, 
                         start_frame: int = 0, num_frames: int = 10,
                         finder: DroneOptimizedPersonFinder = None) -> Dict[str, Any]:
    """åˆ†æè§†é¢‘ç‰‡æ®µ"""
    
    start_time = time.time()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = os.path.join('outputs', f'segment_analysis_{timestamp}')
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ğŸ“¹ å¼€å§‹åˆ†æè§†é¢‘ç‰‡æ®µ...")
    print(f"   è§†é¢‘: {video_path}")
    print(f"   æŸ¥è¯¢: {query_text}")
    print(f"   å¸§èŒƒå›´: {start_frame} - {start_frame + num_frames}")
    print(f"   è¾“å‡º: {output_dir}")
    
    # ä½¿ç”¨ä¼ å…¥çš„æ£€æµ‹å™¨æˆ–åˆ›å»ºé»˜è®¤æ£€æµ‹å™¨
    if finder is None:
        finder = DroneOptimizedPersonFinder()
    
    # åŠ è½½CLIP
    clip_load_start = time.time()
    clip_ctx = load_clip()
    if clip_ctx is None:
        print("âš ï¸ CLIPæœªåŠ è½½ï¼Œå°†ä½¿ç”¨åŸºç¡€æ£€æµ‹åˆ†æ•°")
    else:
        clip_load_time = time.time() - clip_load_start
        print(f"âœ… CLIPåŠ è½½å®Œæˆ ({clip_load_time:.2f}ç§’)")
    
    # æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
        return {}
    
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {total_frames}å¸§, {fps:.1f}FPS")
    
    # è·³è½¬åˆ°èµ·å§‹å¸§
    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
    
    analysis_results = {
        'video_info': {
            'path': video_path,
            'total_frames': total_frames,
            'fps': fps,
            'start_frame': start_frame,
            'num_frames': num_frames
        },
        'query_text': query_text,
        'frame_analyses': [],
        'summary': {},
        'output_dir': output_dir,
        'timing': {
            'start_time': start_time,
            'clip_load_time': clip_load_time if clip_ctx else 0.0
        }
    }
    
    all_detections = []
    all_clip_scores = []
    detection_time = 0.0
    clip_time = 0.0
    
    # é€å¸§åˆ†æ
    for i in range(num_frames):
        ret, frame = cap.read()
        if not ret:
            break
            
        current_frame = start_frame + i
        print(f"\nğŸ” åˆ†æå¸§ {current_frame}...")
        
        # æ£€æµ‹äººç‰©
        det_start = time.time()
        detections = finder.detect_all_persons_in_frame(frame)
        detection_time += time.time() - det_start
        print(f"   æ£€æµ‹åˆ° {len(detections)} ä¸ªäººç‰©")
        
        # CLIPè¯­ä¹‰åˆ†æ
        clip_start = time.time()
        clip_scores = []
        if clip_ctx and query_text:
            # æ›´å¤šæ ·åŒ–çš„æç¤ºæ‰©å±•ï¼Œå¢å¤§åˆ†æ•°å·®å¼‚
            prompts = [query_text]
            text_lower = query_text.lower()
            
            # é€šç”¨æ‰©å±•
            prompts.extend([
                f"a person {query_text}",
                f"person with {query_text}",
                f"person wearing {query_text}"
            ])
            
            # é’ˆå¯¹æ€§æ‰©å±•
            if 'green' in text_lower and ('shirt' in text_lower or 't-shirt' in text_lower):
                prompts.extend([
                    "green t-shirt",
                    "person in green shirt", 
                    "green clothing",
                    "person wearing green top",
                    "green casual wear"
                ])
            elif 'red' in text_lower and 'helmet' in text_lower:
                prompts.extend([
                    "red safety helmet",
                    "construction worker with red helmet",
                    "red hard hat",
                    "person with red protective gear"
                ])
            
            for det in detections:
                crop = det.get('person_crop')
                # è®¡ç®—å¤šæç¤ºçš„æœ€å¤§ç›¸ä¼¼åº¦
                max_score = 0.0
                for prompt in prompts:
                    score = compute_clip_similarity(clip_ctx, crop, prompt)
                    if score is not None:
                        max_score = max(max_score, score)
                clip_scores.append(max_score)
        else:
            clip_scores = [0.0] * len(detections)
        clip_time += time.time() - clip_start
        
        # åˆ›å»ºåˆ†æè¦†ç›–å›¾
        analysis_overlay = create_analysis_overlay(frame, detections, clip_scores, 
                                                 current_frame, query_text)
        
        # ä¿å­˜å¸§åˆ†æç»“æœ
        cv2.imwrite(os.path.join(output_dir, f'frame_{current_frame:04d}_analysis.jpg'), 
                   analysis_overlay)
        
        # åˆ›å»ºè£å‰ªåˆ†æç½‘æ ¼
        create_crop_analysis_grid(detections, clip_scores, query_text, 
                                current_frame, output_dir)
        
        # è®°å½•åˆ†ææ•°æ®
        frame_analysis = {
            'frame_number': current_frame,
            'detection_count': len(detections),
            'detections': [],
            'max_clip_score': max(clip_scores) if clip_scores else 0.0,
            'avg_confidence': np.mean([d['confidence'] for d in detections]) if detections else 0.0
        }
        
        for j, (det, clip_score) in enumerate(zip(detections, clip_scores)):
            det_info = {
                'person_id': j + 1,
                'bbox': det['bbox'],
                'confidence': det['confidence'],
                'size': det['size'],
                'clip_score': clip_score,
                'center': det['center']
            }
            frame_analysis['detections'].append(det_info)
        
        analysis_results['frame_analyses'].append(frame_analysis)
        all_detections.extend(detections)
        all_clip_scores.extend(clip_scores)
    
    cap.release()
    
    total_time = time.time() - start_time
    
    # ç”Ÿæˆæ±‡æ€»åˆ†æ
    if all_detections:
        analysis_results['summary'] = {
            'total_detections': len(all_detections),
            'frames_with_persons': len([f for f in analysis_results['frame_analyses'] if f['detection_count'] > 0]),
            'avg_persons_per_frame': np.mean([f['detection_count'] for f in analysis_results['frame_analyses']]),
            'max_clip_score': max(all_clip_scores) if all_clip_scores else 0.0,
            'avg_clip_score': np.mean(all_clip_scores) if all_clip_scores else 0.0,
            'avg_confidence': np.mean([d['confidence'] for d in all_detections])
        }
    
    # æ·»åŠ æ€§èƒ½ç»Ÿè®¡
    analysis_results['timing'].update({
        'total_time': total_time,
        'detection_time': detection_time,
        'clip_time': clip_time,
        'avg_time_per_frame': total_time / max(1, num_frames),
        'detection_fps': num_frames / max(detection_time, 0.001),
        'clip_fps': len(all_detections) / max(clip_time, 0.001)
    })
    
    # ä¿å­˜JSONæŠ¥å‘Š
    with open(os.path.join(output_dir, 'segment_analysis_report.json'), 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, indent=2, ensure_ascii=False)
    
    # åˆ›å»ºæ±‡æ€»å¯è§†åŒ–
    create_summary_visualization(analysis_results, output_dir)
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"ğŸ” æ£€æµ‹è€—æ—¶: {detection_time:.2f}ç§’ ({num_frames/max(detection_time,0.001):.1f} FPS)")
    print(f"ğŸ§  CLIPè€—æ—¶: {clip_time:.2f}ç§’ ({len(all_detections)/max(clip_time,0.001):.1f} crops/sec)")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ“Š æ€»æ£€æµ‹: {len(all_detections)} ä¸ªäººç‰©")
    print(f"ğŸ¯ æœ€é«˜CLIPåˆ†æ•°: {max(all_clip_scores):.3f}" if all_clip_scores else "")
    print(f"ğŸ“ˆ CLIPåˆ†æ•°èŒƒå›´: {min(all_clip_scores):.3f} - {max(all_clip_scores):.3f}" if all_clip_scores else "")
    
    return analysis_results


def create_summary_visualization(analysis_results: Dict, output_dir: str):
    """åˆ›å»ºæ±‡æ€»å¯è§†åŒ–å›¾è¡¨"""
    frame_analyses = analysis_results['frame_analyses']
    if not frame_analyses:
        return
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle(f'Video Segment Analysis Summary\nQuery: "{analysis_results["query_text"]}"', 
                fontsize=14, fontweight='bold')
    
    # 1. æ¯å¸§æ£€æµ‹æ•°é‡
    frame_nums = [f['frame_number'] for f in frame_analyses]
    detection_counts = [f['detection_count'] for f in frame_analyses]
    
    ax1.bar(range(len(frame_nums)), detection_counts, alpha=0.7, color='skyblue')
    ax1.set_title('Persons Detected per Frame')
    ax1.set_xlabel('Frame Index')
    ax1.set_ylabel('Number of Persons')
    ax1.set_xticks(range(len(frame_nums)))
    ax1.set_xticklabels([f'F{f}' for f in frame_nums], rotation=45)
    
    # 2. CLIPåˆ†æ•°åˆ†å¸ƒ
    all_clip_scores = []
    for f in frame_analyses:
        for det in f['detections']:
            all_clip_scores.append(det['clip_score'])
    
    if all_clip_scores:
        ax2.hist(all_clip_scores, bins=20, alpha=0.7, color='lightcoral')
        ax2.axvline(np.mean(all_clip_scores), color='red', linestyle='--', 
                   label=f'Mean: {np.mean(all_clip_scores):.3f}')
        ax2.set_title('CLIP Score Distribution')
        ax2.set_xlabel('CLIP Score')
        ax2.set_ylabel('Count')
        ax2.legend()
    else:
        ax2.text(0.5, 0.5, 'No CLIP Scores', ha='center', va='center', transform=ax2.transAxes)
        ax2.set_title('CLIP Score Distribution')
    
    # 3. æ£€æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ
    all_confidences = []
    for f in frame_analyses:
        for det in f['detections']:
            all_confidences.append(det['confidence'])
    
    if all_confidences:
        ax3.hist(all_confidences, bins=20, alpha=0.7, color='lightgreen')
        ax3.axvline(np.mean(all_confidences), color='green', linestyle='--',
                   label=f'Mean: {np.mean(all_confidences):.3f}')
        ax3.set_title('Detection Confidence Distribution')
        ax3.set_xlabel('Confidence')
        ax3.set_ylabel('Count')
        ax3.legend()
    
    # 4. äººç‰©å°ºå¯¸åˆ†å¸ƒ
    all_sizes = []
    for f in frame_analyses:
        for det in f['detections']:
            size = det['size']
            all_sizes.append(size[0] * size[1])  # é¢ç§¯
    
    if all_sizes:
        ax4.hist(all_sizes, bins=20, alpha=0.7, color='gold')
        ax4.axvline(np.mean(all_sizes), color='orange', linestyle='--',
                   label=f'Mean: {np.mean(all_sizes):.0f}')
        ax4.set_title('Person Size Distribution (Area)')
        ax4.set_xlabel('Area (pixelsÂ²)')
        ax4.set_ylabel('Count')
        ax4.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'summary_analysis.png'), 
               dpi=150, bbox_inches='tight')
    plt.close()


def create_timeline_view(analysis_results: Dict, output_dir: str):
    """åˆ›å»ºæ—¶é—´è½´è§†å›¾"""
    frame_analyses = analysis_results['frame_analyses']
    if not frame_analyses:
        return
    
    # åˆ›å»ºæ—¶é—´è½´å›¾åƒ
    timeline_height = 200
    frame_width = 150
    total_width = len(frame_analyses) * frame_width
    
    timeline = np.ones((timeline_height, total_width, 3), dtype=np.uint8) * 255
    
    for i, frame_analysis in enumerate(frame_analyses):
        x_start = i * frame_width
        x_end = (i + 1) * frame_width
        
        # ç»˜åˆ¶å¸§ä¿¡æ¯
        frame_num = frame_analysis['frame_number']
        detection_count = frame_analysis['detection_count']
        max_clip = frame_analysis['max_clip_score']
        
        # æ ¹æ®æ£€æµ‹æ•°é‡é€‰æ‹©èƒŒæ™¯é¢œè‰²
        if detection_count == 0:
            bg_color = (240, 240, 240)  # ç°è‰²
        elif detection_count <= 2:
            bg_color = (200, 255, 200)  # æµ…ç»¿
        elif detection_count <= 4:
            bg_color = (255, 255, 200)  # æµ…é»„
        else:
            bg_color = (255, 200, 200)  # æµ…çº¢
        
        cv2.rectangle(timeline, (x_start, 0), (x_end-5, timeline_height), bg_color, -1)
        cv2.rectangle(timeline, (x_start, 0), (x_end-5, timeline_height), (100, 100, 100), 2)
        
        # æ·»åŠ æ–‡å­—ä¿¡æ¯
        texts = [
            f'F{frame_num}',
            f'{detection_count}P',
            f'C:{max_clip:.2f}' if max_clip > 0 else 'C:N/A'
        ]
        
        for j, text in enumerate(texts):
            y_pos = 30 + j * 40
            cv2.putText(timeline, text, (x_start + 10, y_pos),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
    
    cv2.imwrite(os.path.join(output_dir, 'timeline_view.jpg'), timeline)


def create_analysis_video(analysis_results: Dict, output_dir: str, fps: float = 5.0):
    """åˆ›å»ºåˆ†æè¿‡ç¨‹è§†é¢‘"""
    frame_analyses = analysis_results['frame_analyses']
    if not frame_analyses:
        return
    
    print(f"ğŸ¬ ç”Ÿæˆåˆ†æè§†é¢‘...")
    
    # è·å–ç¬¬ä¸€å¸§å°ºå¯¸
    first_frame_path = os.path.join(output_dir, f'frame_{frame_analyses[0]["frame_number"]:04d}_analysis.jpg')
    if not os.path.exists(first_frame_path):
        print("âŒ æ‰¾ä¸åˆ°åˆ†æå¸§å›¾åƒ")
        return
    
    sample_frame = cv2.imread(first_frame_path)
    if sample_frame is None:
        print("âŒ æ— æ³•è¯»å–åˆ†æå¸§å›¾åƒ")
        return
    
    height, width = sample_frame.shape[:2]
    
    # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    video_path = os.path.join(output_dir, 'analysis_video.mp4')
    out = cv2.VideoWriter(video_path, fourcc, fps, (width, height))
    
    if not out.isOpened():
        print("âŒ æ— æ³•åˆ›å»ºè§†é¢‘æ–‡ä»¶")
        return
    
    # å†™å…¥æ¯ä¸€å¸§
    for frame_analysis in frame_analyses:
        frame_num = frame_analysis['frame_number']
        frame_path = os.path.join(output_dir, f'frame_{frame_num:04d}_analysis.jpg')
        
        if os.path.exists(frame_path):
            frame = cv2.imread(frame_path)
            if frame is not None:
                # è°ƒæ•´å°ºå¯¸ç¡®ä¿ä¸€è‡´
                frame = cv2.resize(frame, (width, height))
                out.write(frame)
                print(f"   æ·»åŠ å¸§ {frame_num}")
            else:
                print(f"âš ï¸ æ— æ³•è¯»å–å¸§ {frame_num}")
        else:
            print(f"âš ï¸ æ‰¾ä¸åˆ°å¸§æ–‡ä»¶ {frame_path}")
    
    out.release()
    
    print(f"âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ: {video_path}")
    print(f"ğŸ“¹ è§†é¢‘å‚æ•°: {len(frame_analyses)}å¸§, {fps}FPS, {width}x{height}")


def main():
    ap = ArgumentParser(description='Analyze video segment with frame-by-frame detection and semantic understanding')
    ap.add_argument('video', help='video path')
    ap.add_argument('query', help='query text, e.g., çº¢è‰²å¤´ç›”')
    ap.add_argument('--start-frame', type=int, default=1000, help='starting frame number')
    ap.add_argument('--num-frames', type=int, default=30, help='number of frames to analyze')
    ap.add_argument('--conf', type=float, default=0.15)
    ap.add_argument('--iou', type=float, default=0.35)
    ap.add_argument('--min-size', type=int, default=6)
    ap.add_argument('--tile', type=int, default=960)
    ap.add_argument('--overlap', type=float, default=0.45)
    ap.add_argument('--imgsz', type=int, default=960)
    ap.add_argument('--tta', action='store_true', help='enable TTA for higher recall')
    ap.add_argument('--progressive', action='store_true', help='enable progressive detection for max recall')
    ap.add_argument('--weights', type=str, default='weights/yolov8m.pt', help='YOLO weights')
    ap.add_argument('--output-video', action='store_true', help='generate output video with analysis overlays')
    ap.add_argument('--fps', type=float, default=10.0, help='output video FPS')
    
    args = ap.parse_args()
    
    # æ„å»ºæ£€æµ‹å™¨å‚æ•°
    detector_kwargs = {
        'model_name': args.weights,
        'min_person_size': args.min_size,
        'confidence_threshold': args.conf,
        'nms_threshold': args.iou,
        'tile_size': args.tile,
        'tile_overlap': args.overlap,
        'tile_imgsz': args.imgsz
    }
    
    # è®¾ç½®æ£€æµ‹å™¨å¢å¼ºé€‰é¡¹
    finder = DroneOptimizedPersonFinder(**detector_kwargs)
    setattr(finder, 'use_tta', bool(args.tta))
    setattr(finder, 'use_progressive', bool(args.progressive))
    
    # è¿è¡Œåˆ†æ
    results = analyze_video_segment(
        args.video, args.query, 
        args.start_frame, args.num_frames,
        finder=finder
    )
    
    # åˆ›å»ºæ—¶é—´è½´è§†å›¾
    if results:
        output_dir = results.get('output_dir', 'outputs/segment_analysis_latest')
        create_timeline_view(results, output_dir)
        
        # ç”Ÿæˆè¾“å‡ºè§†é¢‘
        if args.output_video:
            create_analysis_video(results, output_dir, args.fps)
        
        print(f"\nğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   - frame_XXXX_analysis.jpg: é€å¸§è¯¦ç»†åˆ†æ")
        print(f"   - frame_XXXX_crops_analysis.png: äººç‰©è£å‰ªç½‘æ ¼")
        print(f"   - summary_analysis.png: æ±‡æ€»ç»Ÿè®¡å›¾è¡¨")
        print(f"   - timeline_view.jpg: æ—¶é—´è½´è§†å›¾")
        print(f"   - segment_analysis_report.json: è¯¦ç»†æ•°æ®æŠ¥å‘Š")
        if args.output_video:
            print(f"   - analysis_video.mp4: åˆ†æè¿‡ç¨‹è§†é¢‘")


if __name__ == '__main__':
    main() 