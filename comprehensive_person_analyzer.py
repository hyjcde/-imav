#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆäººç‰©åˆ†æå™¨
æ˜¾ç¤ºæ‰€æœ‰æ£€æµ‹åˆ°çš„äººç‰©ï¼Œè¯¦ç»†è®°å½•å’Œåˆ†ææ¯ä¸ªäººä¸æè¿°çš„åŒ¹é…åº¦
"""

import cv2
import numpy as np
from ultralytics import YOLO
from typing import List, Tuple, Dict, Any
import logging
import os
import json
from datetime import datetime
from colorama import Fore, init
import matplotlib.pyplot as plt

init(autoreset=True)
logger = logging.getLogger(__name__)

class ComprehensivePersonAnalyzer:
    """ç»¼åˆäººç‰©åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.detector = self._init_detector()
        
        # æ£€æµ‹å‚æ•°
        self.min_person_size = 15
        self.confidence_threshold = 0.2  # æ›´ä½çš„é˜ˆå€¼ä»¥æ£€æµ‹æ›´å¤šäººç‰©
        self.nms_threshold = 0.3
        
        # åˆ†æè®°å½•
        self.analysis_log = []
        
    def _init_detector(self):
        """åˆå§‹åŒ–YOLOæ£€æµ‹å™¨"""
        try:
            import torch
            import warnings
            warnings.filterwarnings("ignore", category=FutureWarning)
            
            original_load = torch.load
            def patched_load(*args, **kwargs):
                kwargs.setdefault('weights_only', False)
                return original_load(*args, **kwargs)
            torch.load = patched_load
            
            try:
                model = YOLO('weights/yolov8m.pt')
                logger.info("æˆåŠŸåŠ è½½YOLOæ¨¡å‹: yolov8m.pt")
                return model
            finally:
                torch.load = original_load
                
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def detect_all_persons_in_frame(self, frame: np.ndarray) -> List[Dict]:
        """æ£€æµ‹å¸§ä¸­æ‰€æœ‰äººç‰©"""
        detections = []
        original_height, original_width = frame.shape[:2]
        
        # å¤šå°ºåº¦æ£€æµ‹
        scales = [0.7, 0.85, 1.0, 1.2, 1.5, 1.8]  # æ›´å¤šå°ºåº¦
        
        all_boxes = []
        all_scores = []
        all_crops = []
        
        for scale in scales:
            if scale != 1.0:
                new_width = int(original_width * scale)
                new_height = int(original_height * scale)
                scaled_frame = cv2.resize(frame, (new_width, new_height))
            else:
                scaled_frame = frame.copy()
            
            results = self.detector(scaled_frame, 
                                  conf=self.confidence_threshold,
                                  iou=self.nms_threshold,
                                  verbose=False)
            
            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    for box in boxes:
                        class_id = int(box.cls[0])
                        confidence = float(box.conf[0])
                        
                        if class_id == 0:  # person class
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            
                            if scale != 1.0:
                                x1, x2 = x1 / scale, x2 / scale
                                y1, y2 = y1 / scale, y2 / scale
                            
                            x1 = max(0, min(x1, original_width))
                            y1 = max(0, min(y1, original_height))
                            x2 = max(0, min(x2, original_width))
                            y2 = max(0, min(y2, original_height))
                            
                            width = x2 - x1
                            height = y2 - y1
                            
                            if self._is_valid_person(width, height, confidence):
                                all_boxes.append([x1, y1, x2, y2])
                                all_scores.append(confidence)
                                
                                person_crop = frame[int(y1):int(y2), int(x1):int(x2)]
                                all_crops.append(person_crop)
        
        # NMSå¤„ç†
        if all_boxes:
            indices = cv2.dnn.NMSBoxes(
                all_boxes, all_scores, 
                self.confidence_threshold, 
                self.nms_threshold
            )
            
            if len(indices) > 0:
                for i in indices.flatten():
                    x1, y1, x2, y2 = all_boxes[i]
                    detection_info = {
                        'id': len(detections) + 1,
                        'bbox': (int(x1), int(y1), int(x2), int(y2)),
                        'confidence': all_scores[i],
                        'person_crop': all_crops[i],
                        'center': ((int(x1) + int(x2)) // 2, (int(y1) + int(y2)) // 2),
                        'size': (int(x2 - x1), int(y2 - y1)),
                        'area': int((x2 - x1) * (y2 - y1))
                    }
                    detections.append(detection_info)
        
        return detections
    
    def _is_valid_person(self, width: float, height: float, confidence: float) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆäººç‰©æ£€æµ‹"""
        if width < self.min_person_size or height < self.min_person_size:
            return False
        
        # æ”¾å®½å®½é«˜æ¯”é™åˆ¶ï¼Œé€‚åº”æ›´å¤šè§’åº¦
        aspect_ratio = width / height
        if not (0.2 <= aspect_ratio <= 1.2):
            return False
        
        area = width * height
        if area < 150 or area > 8000:  # æ”¾å®½é¢ç§¯é™åˆ¶
            return False
        
        return True
    
    def extract_detailed_color_features(self, person_crop: np.ndarray) -> Dict[str, Any]:
        """æå–è¯¦ç»†çš„é¢œè‰²ç‰¹å¾"""
        if person_crop is None or person_crop.size == 0:
            return {}
        
        height, width = person_crop.shape[:2]
        
        # åˆ†åŒºåŸŸåˆ†æ
        regions = {
            'head': person_crop[:height//3, :],           # å¤´éƒ¨ï¼ˆä¸Š1/3ï¼‰
            'torso': person_crop[height//3:2*height//3, :], # èº¯å¹²ï¼ˆä¸­1/3ï¼‰
            'legs': person_crop[2*height//3:, :],         # è…¿éƒ¨ï¼ˆä¸‹1/3ï¼‰
            'full': person_crop                           # å…¨èº«
        }
        
        color_features = {}
        
        # HSVé¢œè‰²èŒƒå›´å®šä¹‰
        color_ranges = {
            'red': [[(0, 100, 80), (10, 255, 255)], [(170, 100, 80), (180, 255, 255)]],
            'orange': [[(10, 100, 80), (25, 255, 255)]],
            'yellow': [[(25, 100, 80), (35, 255, 255)]],
            'green': [[(35, 100, 80), (85, 255, 255)]],
            'blue': [[(85, 100, 80), (125, 255, 255)]],
            'purple': [[(125, 100, 80), (155, 255, 255)]],
            'pink': [[(155, 100, 80), (170, 255, 255)]],
            'white': [[(0, 0, 180), (180, 30, 255)]],
            'gray': [[(0, 0, 50), (180, 30, 180)]],
            'black': [[(0, 0, 0), (180, 255, 50)]]
        }
        
        for region_name, region_img in regions.items():
            if region_img.size == 0:
                continue
                
            hsv = cv2.cvtColor(region_img, cv2.COLOR_BGR2HSV)
            total_pixels = region_img.shape[0] * region_img.shape[1]
            
            region_colors = {}
            
            for color_name, ranges in color_ranges.items():
                total_mask = np.zeros(hsv.shape[:2], dtype=np.uint8)
                
                for lower, upper in ranges:
                    mask = cv2.inRange(hsv, np.array(lower), np.array(upper))
                    total_mask = cv2.bitwise_or(total_mask, mask)
                
                color_pixels = cv2.countNonZero(total_mask)
                ratio = color_pixels / total_pixels
                
                if ratio > 0.05:  # è‡³å°‘å 5%
                    region_colors[color_name] = ratio
            
            color_features[region_name] = region_colors
        
        return color_features
    
    def analyze_description_match(self, person: Dict, description: str) -> Dict[str, Any]:
        """è¯¦ç»†åˆ†æäººç‰©ä¸æè¿°çš„åŒ¹é…ç¨‹åº¦"""
        # æå–é¢œè‰²ç‰¹å¾
        color_features = self.extract_detailed_color_features(person['person_crop'])
        
        # è§£ææè¿°
        description_lower = description.lower()
        
        # é¢œè‰²å…³é”®è¯
        color_keywords = {
            'red': ['çº¢è‰²', 'çº¢', 'red', 'crimson'],
            'orange': ['æ©™è‰²', 'æ©™', 'orange'],
            'yellow': ['é»„è‰²', 'é»„', 'yellow'],
            'green': ['ç»¿è‰²', 'ç»¿', 'green'],
            'blue': ['è“è‰²', 'è“', 'blue'],
            'purple': ['ç´«è‰²', 'ç´«', 'purple'],
            'pink': ['ç²‰è‰²', 'ç²‰çº¢', 'pink'],
            'white': ['ç™½è‰²', 'ç™½', 'white'],
            'gray': ['ç°è‰²', 'ç°', 'gray', 'grey'],
            'black': ['é»‘è‰²', 'é»‘', 'black']
        }
        
        # æœè£…å…³é”®è¯
        clothing_keywords = {
            'helmet': ['å¤´ç›”', 'å®‰å…¨å¸½', 'helmet', 'hard hat'],
            'hat': ['å¸½å­', 'å¸½', 'hat', 'cap'],
            'shirt': ['ä¸Šè¡£', 'è¡¬è¡«', 'shirt', 'top'],
            'jacket': ['å¤–å¥—', 'å¤¹å…‹', 'jacket', 'coat'],
            'pants': ['è£¤å­', 'é•¿è£¤', 'pants', 'trousers'],
            'shorts': ['çŸ­è£¤', 'shorts'],
            'dress': ['è£™å­', 'è¿è¡£è£™', 'dress']
        }
        
        # è§£æç›®æ ‡ç‰¹å¾
        target_colors = []
        target_clothing = []
        
        for color, keywords in color_keywords.items():
            if any(keyword in description_lower for keyword in keywords):
                target_colors.append(color)
        
        for clothing, keywords in clothing_keywords.items():
            if any(keyword in description_lower for keyword in keywords):
                target_clothing.append(clothing)
        
        # è®¡ç®—åŒ¹é…åˆ†æ•°
        match_analysis = {
            'target_colors': target_colors,
            'target_clothing': target_clothing,
            'detected_colors': color_features,
            'color_matches': {},
            'clothing_matches': {},
            'total_score': 0.0,
            'detailed_scores': {}
        }
        
        # é¢œè‰²åŒ¹é…åˆ†æ
        color_score = 0.0
        if target_colors:
            for target_color in target_colors:
                best_match = 0.0
                best_region = None
                
                for region, colors in color_features.items():
                    if target_color in colors:
                        if colors[target_color] > best_match:
                            best_match = colors[target_color]
                            best_region = region
                
                if best_match > 0:
                    match_analysis['color_matches'][target_color] = {
                        'ratio': best_match,
                        'region': best_region,
                        'score': min(best_match * 2, 1.0)  # è½¬æ¢ä¸º0-1åˆ†æ•°
                    }
                    color_score += match_analysis['color_matches'][target_color]['score']
            
            color_score /= len(target_colors)
        
        # æœè£…åŒ¹é…åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
        clothing_score = 0.5 if target_clothing else 0.0  # åŸºç¡€åˆ†æ•°
        
        # æ£€æµ‹è´¨é‡åˆ†æ•°
        quality_score = person['confidence']
        
        # ç»¼åˆè¯„åˆ†
        total_score = color_score * 0.6 + clothing_score * 0.3 + quality_score * 0.1
        
        match_analysis['detailed_scores'] = {
            'color_score': color_score,
            'clothing_score': clothing_score,
            'quality_score': quality_score
        }
        match_analysis['total_score'] = total_score
        
        return match_analysis
    
    def comprehensive_analysis(self, video_path: str, description: str, max_frames: int = 20) -> Dict[str, Any]:
        """ç»¼åˆåˆ†æè§†é¢‘ä¸­çš„æ‰€æœ‰äººç‰©"""
        print(f"\n{Fore.CYAN}{'='*80}")
        print(f"{Fore.CYAN}ğŸ¯ ç»¼åˆäººç‰©åˆ†æ")
        print(f"{Fore.CYAN}è§†é¢‘: {video_path}")
        print(f"{Fore.CYAN}ç›®æ ‡æè¿°: {description}")
        print(f"{Fore.CYAN}{'='*80}\n")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"{Fore.RED}âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
            return {}
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        skip_frames = max(1, total_frames // max_frames)
        
        output_dir = os.path.join("outputs", "comprehensive_analysis")
        os.makedirs(output_dir, exist_ok=True)
        
        all_detections = []
        frame_results = []
        frame_count = 0
        
        print(f"{Fore.YELLOW}ğŸ“Š å¼€å§‹åˆ†æ {max_frames} å¸§...")
        
        while cap.isOpened() and frame_count < max_frames:
            ret, frame = cap.read()
            if not ret:
                break
            
            current_frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES))
            if current_frame_number % skip_frames != 0:
                continue
            
            # æ£€æµ‹æ‰€æœ‰äººç‰©
            detections = self.detect_all_persons_in_frame(frame)
            
            if detections:
                print(f"\n{Fore.GREEN}ğŸ“ å¸§ {current_frame_number}: æ£€æµ‹åˆ° {len(detections)} ä¸ªäººç‰©")
                
                frame_analysis = {
                    'frame_number': current_frame_number,
                    'detections': [],
                    'best_match': None,
                    'frame': frame.copy()
                }
                
                # åˆ†ææ¯ä¸ªæ£€æµ‹åˆ°çš„äººç‰©
                for person in detections:
                    match_analysis = self.analyze_description_match(person, description)
                    
                    person_analysis = {
                        'person_id': person['id'],
                        'bbox': person['bbox'],
                        'confidence': person['confidence'],
                        'size': person['size'],
                        'area': person['area'],
                        'match_analysis': match_analysis
                    }
                    
                    frame_analysis['detections'].append(person_analysis)
                    all_detections.append(person_analysis)
                    
                    # æ‰“å°è¯¦ç»†åˆ†æ
                    print(f"  ğŸ‘¤ äººç‰© {person['id']}:")
                    print(f"     ä½ç½®: {person['bbox']}")
                    print(f"     å¤§å°: {person['size'][0]}x{person['size'][1]}")
                    print(f"     ç½®ä¿¡åº¦: {person['confidence']:.3f}")
                    print(f"     åŒ¹é…åˆ†æ•°: {match_analysis['total_score']:.3f}")
                    
                    if match_analysis['color_matches']:
                        print(f"     é¢œè‰²åŒ¹é…:")
                        for color, match_info in match_analysis['color_matches'].items():
                            print(f"       {color}: {match_info['ratio']:.2%} ({match_info['region']})")
                
                # æ‰¾å‡ºæœ€ä½³åŒ¹é…
                if frame_analysis['detections']:
                    best_detection = max(frame_analysis['detections'], 
                                       key=lambda x: x['match_analysis']['total_score'])
                    frame_analysis['best_match'] = best_detection
                    
                    print(f"  ğŸ† æœ€ä½³åŒ¹é…: äººç‰© {best_detection['person_id']} (åˆ†æ•°: {best_detection['match_analysis']['total_score']:.3f})")
                
                # å¯è§†åŒ–å¹¶ä¿å­˜
                vis_frame = self.visualize_all_detections(frame, frame_analysis, description)
                output_path = f"{output_dir}/frame_{current_frame_number}_analysis.jpg"
                cv2.imwrite(output_path, vis_frame)
                
                frame_results.append(frame_analysis)
            
            frame_count += 1
        
        cap.release()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = self.generate_comprehensive_report(all_detections, frame_results, description, output_dir)
        
        print(f"\n{Fore.CYAN}{'='*80}")
        print(f"{Fore.CYAN}ğŸ“‹ åˆ†æå®Œæˆ")
        print(f"{Fore.GREEN}æ€»è®¡æ£€æµ‹åˆ°: {len(all_detections)} ä¸ªäººç‰©å®ä¾‹")
        print(f"{Fore.GREEN}åŒ…å«äººç‰©çš„å¸§æ•°: {len(frame_results)}")
        print(f"{Fore.GREEN}ç»“æœä¿å­˜åœ¨: {output_dir}")
        print(f"{Fore.CYAN}{'='*80}")
        
        return report
    
    def visualize_all_detections(self, frame: np.ndarray, frame_analysis: Dict, description: str) -> np.ndarray:
        """å¯è§†åŒ–æ‰€æœ‰æ£€æµ‹ç»“æœ"""
        vis_frame = frame.copy()
        detections = frame_analysis['detections']
        
        # ä¸ºæ¯ä¸ªäººç‰©åˆ†é…ä¸åŒé¢œè‰²
        colors = [
            (255, 0, 0),    # è“è‰²
            (0, 255, 0),    # ç»¿è‰²
            (0, 0, 255),    # çº¢è‰²
            (255, 255, 0),  # é’è‰²
            (255, 0, 255),  # æ´‹çº¢
            (0, 255, 255),  # é»„è‰²
            (128, 0, 128),  # ç´«è‰²
            (255, 165, 0),  # æ©™è‰²
        ]
        
        for i, detection in enumerate(detections):
            x1, y1, x2, y2 = detection['bbox']
            match_score = detection['match_analysis']['total_score']
            
            # æ ¹æ®åŒ¹é…åˆ†æ•°é€‰æ‹©é¢œè‰²å¼ºåº¦
            if match_score > 0.5:
                color = (0, 255, 0)  # é«˜åˆ†ç”¨ç»¿è‰²
                thickness = 4
            elif match_score > 0.3:
                color = (0, 255, 255)  # ä¸­åˆ†ç”¨é»„è‰²
                thickness = 3
            else:
                color = colors[i % len(colors)]  # ä½åˆ†ç”¨ä¸åŒé¢œè‰²
                thickness = 2
            
            # ç»˜åˆ¶æ£€æµ‹æ¡†
            cv2.rectangle(vis_frame, (x1, y1), (x2, y2), color, thickness)
            
            # æ·»åŠ è¯¦ç»†æ ‡ç­¾
            labels = [
                f"Person {detection['person_id']}",
                f"Conf: {detection['confidence']:.3f}",
                f"Match: {match_score:.3f}",
                f"Size: {detection['size'][0]}x{detection['size'][1]}"
            ]
            
            # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
            label_height = len(labels) * 20 + 10
            cv2.rectangle(vis_frame, (x1, y1 - label_height), (x1 + 200, y1), color, -1)
            
            # ç»˜åˆ¶æ–‡å­—
            for j, label in enumerate(labels):
                y_pos = y1 - label_height + 15 + j * 20
                cv2.putText(vis_frame, label, (x1 + 5, y_pos),
                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)
            
            # æ˜¾ç¤ºé¢œè‰²åŒ¹é…ä¿¡æ¯
            if detection['match_analysis']['color_matches']:
                y_offset = y2 + 20
                for color_name, match_info in detection['match_analysis']['color_matches'].items():
                    color_text = f"{color_name}: {match_info['ratio']:.1%} ({match_info['region']})"
                    cv2.putText(vis_frame, color_text, (x1, y_offset),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
                    y_offset += 15
        
        # æ·»åŠ å¸§ä¿¡æ¯
        frame_info = [
            f"Frame: {frame_analysis['frame_number']}",
            f"Persons: {len(detections)}",
            f"Target: {description}",
            f"Best Match: {frame_analysis['best_match']['match_analysis']['total_score']:.3f}" if frame_analysis['best_match'] else "No Match"
        ]
        
        for i, info in enumerate(frame_info):
            cv2.putText(vis_frame, info, (10, 30 + i * 25),
                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        return vis_frame
    
    def generate_comprehensive_report(self, all_detections: List[Dict], frame_results: List[Dict], 
                                    description: str, output_dir: str) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        # æŒ‰åŒ¹é…åˆ†æ•°æ’åº
        sorted_detections = sorted(all_detections, key=lambda x: x['match_analysis']['total_score'], reverse=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_detections = len(all_detections)
        total_frames = len(frame_results)
        avg_score = np.mean([d['match_analysis']['total_score'] for d in all_detections])
        
        # æœ€ä½³åŒ¹é…åˆ†æ
        best_matches = sorted_detections[:10]  # å‰10ä¸ªæœ€ä½³åŒ¹é…
        
        # ç”ŸæˆJSONæŠ¥å‘Š
        report = {
            'analysis_info': {
                'timestamp': datetime.now().isoformat(),
                'description': description,
                'total_detections': total_detections,
                'total_frames': total_frames,
                'average_score': float(avg_score)
            },
            'best_matches': [],
            'frame_summary': []
        }
        
        # æœ€ä½³åŒ¹é…è¯¦æƒ…
        for i, detection in enumerate(best_matches):
            match_info = {
                'rank': i + 1,
                'person_id': detection['person_id'],
                'total_score': detection['match_analysis']['total_score'],
                'bbox': detection['bbox'],
                'size': detection['size'],
                'confidence': detection['confidence'],
                'color_matches': detection['match_analysis']['color_matches'],
                'detailed_scores': detection['match_analysis']['detailed_scores']
            }
            report['best_matches'].append(match_info)
        
        # å¸§æ±‡æ€»
        for frame_result in frame_results:
            frame_summary = {
                'frame_number': frame_result['frame_number'],
                'person_count': len(frame_result['detections']),
                'best_score': frame_result['best_match']['match_analysis']['total_score'] if frame_result['best_match'] else 0.0
            }
            report['frame_summary'].append(frame_summary)
        
        # ä¿å­˜JSONæŠ¥å‘Š
        with open(f"{output_dir}/analysis_report.json", 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        self.create_visual_report(report, output_dir)
        
        # æ‰“å°æœ€ä½³åŒ¹é…ç»“æœ
        print(f"\n{Fore.CYAN}ğŸ† å‰10ä¸ªæœ€ä½³åŒ¹é…:")
        for match in best_matches:
            print(f"  {match['match_analysis']['total_score']:.3f} - äººç‰©ID {match['person_id']} "
                  f"(å¤§å°: {match['size'][0]}x{match['size'][1]})")
            if match['match_analysis']['color_matches']:
                for color, info in match['match_analysis']['color_matches'].items():
                    print(f"    {color}: {info['ratio']:.1%} åœ¨ {info['region']}")
        
        return report
    
    def create_visual_report(self, report: Dict, output_dir: str):
        """åˆ›å»ºå¯è§†åŒ–æŠ¥å‘Š"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'äººç‰©æ£€æµ‹åˆ†ææŠ¥å‘Š - {report["analysis_info"]["description"]}', 
                    fontsize=16, fontweight='bold')
        
        # 1. åŒ¹é…åˆ†æ•°åˆ†å¸ƒ
        scores = [m['total_score'] for m in report['best_matches']]
        ax1.bar(range(len(scores)), scores, alpha=0.7, color='skyblue')
        ax1.set_title('æœ€ä½³åŒ¹é…åˆ†æ•°åˆ†å¸ƒ')
        ax1.set_xlabel('æ’å')
        ax1.set_ylabel('åŒ¹é…åˆ†æ•°')
        ax1.set_xticks(range(len(scores)))
        ax1.set_xticklabels([f'#{i+1}' for i in range(len(scores))])
        
        # 2. æ¯å¸§äººç‰©æ•°é‡
        frame_nums = [f['frame_number'] for f in report['frame_summary']]
        person_counts = [f['person_count'] for f in report['frame_summary']]
        ax2.plot(frame_nums, person_counts, 'o-', color='orange')
        ax2.set_title('æ¯å¸§æ£€æµ‹äººç‰©æ•°é‡')
        ax2.set_xlabel('å¸§å·')
        ax2.set_ylabel('äººç‰©æ•°é‡')
        ax2.grid(True, alpha=0.3)
        
        # 3. äººç‰©å¤§å°åˆ†å¸ƒ
        sizes = [m['size'][0] * m['size'][1] for m in report['best_matches']]
        ax3.hist(sizes, bins=10, alpha=0.7, color='lightgreen')
        ax3.set_title('äººç‰©å¤§å°åˆ†å¸ƒ (åƒç´ Â²)')
        ax3.set_xlabel('é¢ç§¯ (åƒç´ Â²)')
        ax3.set_ylabel('æ•°é‡')
        
        # 4. ç½®ä¿¡åº¦vsåŒ¹é…åˆ†æ•°
        confidences = [m['confidence'] for m in report['best_matches']]
        match_scores = [m['total_score'] for m in report['best_matches']]
        ax4.scatter(confidences, match_scores, alpha=0.7, color='red')
        ax4.set_title('æ£€æµ‹ç½®ä¿¡åº¦ vs åŒ¹é…åˆ†æ•°')
        ax4.set_xlabel('æ£€æµ‹ç½®ä¿¡åº¦')
        ax4.set_ylabel('åŒ¹é…åˆ†æ•°')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/visual_report.png", dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"{Fore.GREEN}ğŸ“Š å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜: visual_report.png")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 3:
        video_path = "data/DJI_20250807124730_0002_S.MP4"
        description = "a man with red helmet"
        max_frames = 15
        print(f"{Fore.CYAN}ä½¿ç”¨é»˜è®¤å‚æ•°: {video_path}, '{description}'")
    else:
        video_path = sys.argv[1]
        description = sys.argv[2]
        max_frames = int(sys.argv[3]) if len(sys.argv) > 3 else 15
    
    try:
        analyzer = ComprehensivePersonAnalyzer()
        report = analyzer.comprehensive_analysis(video_path, description, max_frames)
        
    except Exception as e:
        print(f"{Fore.RED}âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 