#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨äººç‰©æŸ¥æ‰¾ç³»ç»Ÿ
æ”¯æŒä»»æ„æ–‡æœ¬æè¿°ï¼Œæä¾›é«˜è´¨é‡å¯è§†åŒ–å’Œç»“æœä¿å­˜
åŸºäºYOLOv8æ£€æµ‹å’Œæ™ºèƒ½æ–‡æœ¬åŒ¹é…
"""

import cv2
import numpy as np
from ultralytics import YOLO
from typing import List, Tuple, Dict, Any, Optional
import logging
import os
import json
from datetime import datetime
from colorama import Fore, Style, init
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib import font_manager
import re
from utils.tiling import run_tiled_detection, nms_boxes
from utils.yolo_utils import safe_predict

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

init(autoreset=True)
logger = logging.getLogger(__name__)

class UniversalPersonFinder:
    """é€šç”¨äººç‰©æŸ¥æ‰¾ç³»ç»Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        self.detector = self._init_detector()
        
        # æ£€æµ‹å‚æ•°ï¼ˆæ›´ä½é˜ˆå€¼+æ›´å°å°ºå¯¸ä»¥æ•æ‰æ›´å¤šè¡Œäººï¼‰
        self.min_person_size = 10
        self.confidence_threshold = 0.20
        self.nms_threshold = 0.4
        
        # æ‰©å±•é¢œè‰²è¯å…¸
        self.color_keywords = {
            'red': {
                'cn': ['çº¢è‰²', 'çº¢', 'èµ¤è‰²', 'æœ±çº¢', 'å¤§çº¢', 'æ·±çº¢', 'æµ…çº¢'],
                'en': ['red', 'crimson', 'scarlet', 'burgundy', 'maroon']
            },
            'blue': {
                'cn': ['è“è‰²', 'è“', 'æ·±è“', 'æµ…è“', 'å¤©è“', 'æµ·è“'],
                'en': ['blue', 'navy', 'cyan', 'azure', 'cobalt']
            },
            'green': {
                'cn': ['ç»¿è‰²', 'ç»¿', 'æ·±ç»¿', 'æµ…ç»¿', 'ç¿ ç»¿', 'è‰ç»¿'],
                'en': ['green', 'lime', 'forest', 'olive', 'emerald']
            },
            'yellow': {
                'cn': ['é»„è‰²', 'é»„', 'é‡‘é»„', 'æ·¡é»„', 'äº®é»„'],
                'en': ['yellow', 'gold', 'amber', 'lemon']
            },
            'orange': {
                'cn': ['æ©™è‰²', 'æ©™', 'æ¡”è‰²', 'æ©˜è‰²'],
                'en': ['orange', 'tangerine', 'peach']
            },
            'purple': {
                'cn': ['ç´«è‰²', 'ç´«', 'æ·±ç´«', 'æµ…ç´«'],
                'en': ['purple', 'violet', 'lavender', 'magenta']
            },
            'white': {
                'cn': ['ç™½è‰²', 'ç™½', 'çº¯ç™½', 'ç±³ç™½'],
                'en': ['white', 'ivory', 'cream']
            },
            'black': {
                'cn': ['é»‘è‰²', 'é»‘', 'æ·±é»‘'],
                'en': ['black', 'dark']
            },
            'gray': {
                'cn': ['ç°è‰²', 'ç°', 'æ·±ç°', 'æµ…ç°'],
                'en': ['gray', 'grey', 'silver']
            },
            'brown': {
                'cn': ['æ£•è‰²', 'è¤è‰²', 'å’–å•¡è‰²', 'åœŸé»„'],
                'en': ['brown', 'tan', 'beige', 'khaki']
            },
            'pink': {
                'cn': ['ç²‰è‰²', 'ç²‰çº¢', 'æ¨±èŠ±è‰²'],
                'en': ['pink', 'rose']
            }
        }
        
        # æ‰©å±•æœè£…è¯å…¸
        self.clothing_keywords = {
            'helmet': {
                'cn': ['å¤´ç›”', 'å®‰å…¨å¸½', 'å·¥åœ°å¸½', 'é˜²æŠ¤å¸½'],
                'en': ['helmet', 'hard hat', 'safety helmet']
            },
            'hat': {
                'cn': ['å¸½å­', 'å¸½', 'æ£’çƒå¸½', 'é®é˜³å¸½'],
                'en': ['hat', 'cap', 'baseball cap']
            },
            'shirt': {
                'cn': ['ä¸Šè¡£', 'è¡¬è¡«', 'ä½“æ¤', 'Tæ¤', 'çŸ­è¢–'],
                'en': ['shirt', 't-shirt', 'top', 'blouse']
            },
            'jacket': {
                'cn': ['å¤–å¥—', 'å¤¹å…‹', 'é£è¡£', 'å¤§è¡£'],
                'en': ['jacket', 'coat', 'windbreaker']
            },
            'vest': {
                'cn': ['èƒŒå¿ƒ', 'é©¬ç”²', 'åå…‰èƒŒå¿ƒ'],
                'en': ['vest', 'waistcoat', 'safety vest']
            },
            'pants': {
                'cn': ['è£¤å­', 'é•¿è£¤', 'å·¥è£…è£¤'],
                'en': ['pants', 'trousers', 'jeans']
            },
            'shorts': {
                'cn': ['çŸ­è£¤'],
                'en': ['shorts']
            },
            'uniform': {
                'cn': ['åˆ¶æœ', 'å·¥è£…', 'å·¥ä½œæœ'],
                'en': ['uniform', 'workwear']
            }
        }
        
        # ä½ç½®å’ŒåŠ¨ä½œè¯å…¸
        self.position_keywords = {
            'standing': {
                'cn': ['ç«™ç€', 'ç«™ç«‹', 'ç›´ç«‹'],
                'en': ['standing', 'upright']
            },
            'walking': {
                'cn': ['èµ°è·¯', 'è¡Œèµ°', 'æ­¥è¡Œ'],
                'en': ['walking', 'moving']
            },
            'sitting': {
                'cn': ['åç€', 'åä¸‹'],
                'en': ['sitting', 'seated']
            },
            'working': {
                'cn': ['å·¥ä½œ', 'åŠ³åŠ¨', 'æ–½å·¥'],
                'en': ['working', 'laboring']
            },
            'center': {
                'cn': ['ä¸­é—´', 'ä¸­å¤®', 'æ­£ä¸­'],
                'en': ['center', 'middle', 'central']
            },
            'left': {
                'cn': ['å·¦è¾¹', 'å·¦ä¾§'],
                'en': ['left', 'left side']
            },
            'right': {
                'cn': ['å³è¾¹', 'å³ä¾§'],
                'en': ['right', 'right side']
            },
            'alone': {
                'cn': ['å•ç‹¬', 'ä¸€ä¸ªäºº', 'ç‹¬è‡ª'],
                'en': ['alone', 'single', 'individual']
            },
            'group': {
                'cn': ['ç¾¤ä½“', 'å¤šäºº', 'ä¸€ç¾¤'],
                'en': ['group', 'multiple', 'crowd']
            }
        }
        
    def _init_detector(self):
        """åˆå§‹åŒ–YOLOæ£€æµ‹å™¨"""
        try:
            import torch
            import warnings
            warnings.filterwarnings("ignore", category=FutureWarning)
            
            original_load = torch.load
            def patched_load(*args, **kwargs):
                kwargs.setdefault('weights_only', False)
                return original_load(*args, **kwargs)
            torch.load = patched_load
            
            try:
                model = YOLO('weights/yolov8m.pt')
                logger.info("æˆåŠŸåŠ è½½YOLOæ¨¡å‹: yolov8m.pt")
                return model
            finally:
                torch.load = original_load
                
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def detect_persons_in_frame(self, frame: np.ndarray) -> List[Dict]:
        """æ£€æµ‹å¸§ä¸­çš„æ‰€æœ‰äººç‰©"""
        detections = []
        original_height, original_width = frame.shape[:2]

        # å…ˆè¿›è¡Œåˆ‡ç‰‡æ¨ç†ï¼Œæå‡å°ç›®æ ‡å¬å›ï¼ˆèˆªæ‹å‹å¥½ï¼‰
        tiled_boxes, tiled_scores = run_tiled_detection(
            self.detector,
            frame,
            conf_th=self.confidence_threshold,
            iou_th=self.nms_threshold,
            tile_size=(960, 960),
            overlap_ratio=0.3,
            imgsz=960
        )
        keep_idx = nms_boxes(tiled_boxes, tiled_scores, self.confidence_threshold, self.nms_threshold)
        for i in keep_idx:
            x1, y1, x2, y2 = tiled_boxes[i]
            confidence = float(tiled_scores[i])
            person_crop = frame[int(y1):int(y2), int(x1):int(x2)]
            detections.append({
                'id': len(detections) + 1,
                'bbox': (int(x1), int(y1), int(x2), int(y2)),
                'confidence': confidence,
                'person_crop': person_crop,
                'center': ((int(x1) + int(x2)) // 2, (int(y1) + int(y2)) // 2),
                'size': (int(x2 - x1), int(y2 - y1)),
                'area': int((x2 - x1) * (y2 - y1))
            })

        if detections:
            return detections

        # å›é€€ï¼šåŸå¤šå°ºåº¦ï¼ˆä¿è¯å…¼å®¹ï¼‰
        scales = [0.8, 1.0, 1.3, 1.6]
        all_boxes = []
        all_scores = []
        all_crops = []
        for scale in scales:
            if scale != 1.0:
                new_width = int(original_width * scale)
                new_height = int(original_height * scale)
                scaled_frame = cv2.resize(frame, (new_width, new_height))
            else:
                scaled_frame = frame.copy()
            results = safe_predict(self.detector, scaled_frame,
                                  conf=self.confidence_threshold,
                                  iou=self.nms_threshold,
                                  verbose=False)
            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    for box in boxes:
                        class_id = int(box.cls[0])
                        confidence = float(box.conf[0])
                        if class_id == 0:
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            if scale != 1.0:
                                x1, x2 = x1 / scale, x2 / scale
                                y1, y2 = y1 / scale, y2 / scale
                            x1 = max(0, min(x1, original_width))
                            y1 = max(0, min(y1, original_height))
                            x2 = max(0, min(x2, original_width))
                            y2 = max(0, min(y2, original_height))
                            width = x2 - x1
                            height = y2 - y1
                            if self._is_valid_person(width, height, confidence):
                                all_boxes.append([x1, y1, x2, y2])
                                all_scores.append(confidence)
                                person_crop = frame[int(y1):int(y2), int(x1):int(x2)]
                                all_crops.append(person_crop)
        if all_boxes:
            indices = cv2.dnn.NMSBoxes(all_boxes, all_scores,
                                       self.confidence_threshold,
                                       self.nms_threshold)
            if len(indices) > 0:
                for i in indices.flatten():
                    x1, y1, x2, y2 = all_boxes[i]
                    detections.append({
                        'id': len(detections) + 1,
                        'bbox': (int(x1), int(y1), int(x2), int(y2)),
                        'confidence': all_scores[i],
                        'person_crop': all_crops[i],
                        'center': ((int(x1) + int(x2)) // 2, (int(y1) + int(y2)) // 2),
                        'size': (int(x2 - x1), int(y2 - y1)),
                        'area': int((x2 - x1) * (y2 - y1))
                    })
        return detections
    
    def _is_valid_person(self, width: float, height: float, confidence: float) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆäººç‰©æ£€æµ‹"""
        if width < self.min_person_size or height < self.min_person_size:
            return False
        
        aspect_ratio = width / height
        if not (0.2 <= aspect_ratio <= 1.5):
            return False
        
        area = width * height
        if area < 200 or area > 12000:
            return False
        
        return True
    
    def extract_color_features(self, person_crop: np.ndarray) -> Dict[str, Dict[str, float]]:
        """æå–é¢œè‰²ç‰¹å¾"""
        if person_crop is None or person_crop.size == 0:
            return {}
        
        height, width = person_crop.shape[:2]
        
        # åˆ†åŒºåŸŸåˆ†æï¼ˆå¤´éƒ¨ã€èº¯å¹²ã€è…¿éƒ¨ï¼‰
        regions = {
            'head': person_crop[:height//3, :],           
            'torso': person_crop[height//3:2*height//3, :], 
            'legs': person_crop[2*height//3:, :],         
            'full': person_crop                           
        }
        
        # HSVé¢œè‰²èŒƒå›´
        color_ranges = {
            'red': [[(0, 120, 70), (10, 255, 255)], [(170, 120, 70), (180, 255, 255)]],
            'blue': [[(100, 120, 70), (130, 255, 255)]],
            'green': [[(40, 120, 70), (80, 255, 255)]],
            'yellow': [[(20, 120, 70), (30, 255, 255)]],
            'orange': [[(10, 120, 70), (20, 255, 255)]],
            'purple': [[(130, 120, 70), (160, 255, 255)]],
            'white': [[(0, 0, 200), (180, 30, 255)]],
            'black': [[(0, 0, 0), (180, 255, 70)]],
            'gray': [[(0, 0, 70), (180, 30, 200)]],
            'brown': [[(10, 50, 20), (20, 255, 200)]],
            'pink': [[(160, 120, 70), (170, 255, 255)]]
        }
        
        color_features = {}
        
        for region_name, region_img in regions.items():
            if region_img.size == 0:
                continue
                
            hsv = cv2.cvtColor(region_img, cv2.COLOR_BGR2HSV)
            total_pixels = region_img.shape[0] * region_img.shape[1]
            
            region_colors = {}
            
            for color_name, ranges in color_ranges.items():
                total_mask = np.zeros(hsv.shape[:2], dtype=np.uint8)
                
                for lower, upper in ranges:
                    mask = cv2.inRange(hsv, np.array(lower), np.array(upper))
                    total_mask = cv2.bitwise_or(total_mask, mask)
                
                color_pixels = cv2.countNonZero(total_mask)
                ratio = color_pixels / total_pixels
                
                if ratio > 0.05:  # è‡³å°‘å 5%
                    region_colors[color_name] = ratio
            
            color_features[region_name] = region_colors
        
        return color_features
    
    def parse_text_description(self, description: str) -> Dict[str, List[str]]:
        """è§£ææ–‡æœ¬æè¿°ï¼Œæå–å…³é”®ç‰¹å¾"""
        description_lower = description.lower()
        
        parsed_features = {
            'colors': [],
            'clothing': [],
            'positions': [],
            'raw_text': description
        }
        
        # æå–é¢œè‰²
        for color, keywords in self.color_keywords.items():
            all_keywords = keywords['cn'] + keywords['en']
            if any(keyword in description_lower for keyword in all_keywords):
                parsed_features['colors'].append(color)
        
        # æå–æœè£…
        for clothing, keywords in self.clothing_keywords.items():
            all_keywords = keywords['cn'] + keywords['en']
            if any(keyword in description_lower for keyword in all_keywords):
                parsed_features['clothing'].append(clothing)
        
        # æå–ä½ç½®/åŠ¨ä½œ
        for position, keywords in self.position_keywords.items():
            all_keywords = keywords['cn'] + keywords['en']
            if any(keyword in description_lower for keyword in all_keywords):
                parsed_features['positions'].append(position)
        
        return parsed_features
    
    def calculate_match_score(self, person: Dict, description_features: Dict) -> Dict[str, Any]:
        """è®¡ç®—åŒ¹é…åˆ†æ•°"""
        color_features = self.extract_color_features(person['person_crop'])
        
        match_analysis = {
            'color_score': 0.0,
            'clothing_score': 0.0,
            'position_score': 0.0,
            'quality_score': person['confidence'],
            'total_score': 0.0,
            'matched_features': {
                'colors': {},
                'clothing': [],
                'positions': []
            }
        }
        
        # é¢œè‰²åŒ¹é…åˆ†æ
        if description_features['colors']:
            color_matches = 0
            total_color_score = 0.0
            
            for target_color in description_features['colors']:
                best_match = 0.0
                best_region = None
                
                for region, colors in color_features.items():
                    if target_color in colors:
                        if colors[target_color] > best_match:
                            best_match = colors[target_color]
                            best_region = region
                
                if best_match > 0.05:  # è‡³å°‘5%çš„é¢œè‰²åŒ¹é…
                    color_matches += 1
                    score = min(best_match * 2, 1.0)
                    total_color_score += score
                    match_analysis['matched_features']['colors'][target_color] = {
                        'ratio': best_match,
                        'region': best_region,
                        'score': score
                    }
            
            if color_matches > 0:
                match_analysis['color_score'] = total_color_score / len(description_features['colors'])
        
        # æœè£…åŒ¹é…ï¼ˆåŸºç¡€åˆ†æ•°ï¼‰
        if description_features['clothing']:
            match_analysis['clothing_score'] = 0.6  # åŸºç¡€æœè£…åˆ†æ•°
            match_analysis['matched_features']['clothing'] = description_features['clothing']
        
        # ä½ç½®åŒ¹é…ï¼ˆç®€åŒ–ï¼‰
        if description_features['positions']:
            match_analysis['position_score'] = 0.5  # åŸºç¡€ä½ç½®åˆ†æ•°
            match_analysis['matched_features']['positions'] = description_features['positions']
        
        # ç»¼åˆè¯„åˆ†
        weights = {
            'color': 0.5 if description_features['colors'] else 0.0,
            'clothing': 0.3 if description_features['clothing'] else 0.0,
            'position': 0.1 if description_features['positions'] else 0.0,
            'quality': 0.1
        }
        
        # é‡æ–°åˆ†é…æƒé‡
        total_weight = sum(weights.values())
        if total_weight > 0:
            for key in weights:
                weights[key] /= total_weight
        
        match_analysis['total_score'] = (
            match_analysis['color_score'] * weights['color'] +
            match_analysis['clothing_score'] * weights['clothing'] +
            match_analysis['position_score'] * weights['position'] +
            match_analysis['quality_score'] * weights['quality']
        )
        
        return match_analysis
    
    def find_best_matches(self, video_path: str, description: str, max_frames: int = 20) -> Dict[str, Any]:
        """æŸ¥æ‰¾æœ€ä½³åŒ¹é…"""
        print(f"\n{Fore.CYAN}{'='*80}")
        print(f"{Fore.CYAN}ğŸ” é€šç”¨äººç‰©æŸ¥æ‰¾ç³»ç»Ÿ")
        print(f"{Fore.CYAN}è§†é¢‘: {video_path}")
        print(f"{Fore.CYAN}ç›®æ ‡æè¿°: {Style.BRIGHT}{description}")
        print(f"{Fore.CYAN}{'='*80}\n")
        
        # è§£ææè¿°
        description_features = self.parse_text_description(description)
        print(f"{Fore.YELLOW}ğŸ“ è§£æçš„ç‰¹å¾:")
        print(f"   é¢œè‰²: {description_features['colors']}")
        print(f"   æœè£…: {description_features['clothing']}")
        print(f"   ä½ç½®/åŠ¨ä½œ: {description_features['positions']}\n")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"{Fore.RED}âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
            return {}
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        skip_frames = max(1, total_frames // max_frames)
        
        output_dir = os.path.join("outputs", f"search_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        os.makedirs(output_dir, exist_ok=True)
        
        all_matches = []
        frame_count = 0
        
        print(f"{Fore.YELLOW}ğŸ” å¼€å§‹æœç´¢ {max_frames} å¸§...")
        
        while cap.isOpened() and frame_count < max_frames:
            ret, frame = cap.read()
            if not ret:
                break
            
            current_frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES))
            if current_frame_number % skip_frames != 0:
                continue
            
            # æ£€æµ‹äººç‰©
            detections = self.detect_persons_in_frame(frame)
            
            if detections:
                print(f"\n{Fore.GREEN}ğŸ“ å¸§ {current_frame_number}: å‘ç° {len(detections)} ä¸ªäººç‰©")
                
                for person in detections:
                    match_analysis = self.calculate_match_score(person, description_features)
                    
                    person_result = {
                        'frame_number': current_frame_number,
                        'person_id': person['id'],
                        'bbox': person['bbox'],
                        'confidence': person['confidence'],
                        'size': person['size'],
                        'match_analysis': match_analysis,
                        'frame': frame.copy()
                    }
                    
                    all_matches.append(person_result)
                    
                    print(f"  ğŸ‘¤ äººç‰© {person['id']}: åŒ¹é…åˆ†æ•° {match_analysis['total_score']:.3f}")
                    if match_analysis['matched_features']['colors']:
                        for color, info in match_analysis['matched_features']['colors'].items():
                            print(f"     {color}: {info['ratio']:.1%} ({info['region']})")
            
            frame_count += 1
        
        cap.release()
        
        # æ’åºå¹¶è·å–æœ€ä½³åŒ¹é…
        all_matches.sort(key=lambda x: x['match_analysis']['total_score'], reverse=True)
        
        # ä¿å­˜ç»“æœ
        result_summary = self.save_search_results(all_matches, description_features, output_dir)
        
        print(f"\n{Fore.CYAN}{'='*60}")
        print(f"{Fore.GREEN}ğŸ¯ æœç´¢å®Œæˆï¼")
        print(f"{Fore.GREEN}æ€»æ£€æµ‹: {len(all_matches)} ä¸ªäººç‰©")
        if all_matches:
            print(f"{Fore.GREEN}æœ€ä½³åŒ¹é…åˆ†æ•°: {all_matches[0]['match_analysis']['total_score']:.3f}")
        print(f"{Fore.GREEN}ç»“æœä¿å­˜åœ¨: {output_dir}")
        print(f"{Fore.CYAN}{'='*60}")
        
        return result_summary
    
    def save_search_results(self, all_matches: List[Dict], description_features: Dict, output_dir: str) -> Dict[str, Any]:
        """ä¿å­˜æœç´¢ç»“æœ"""
        if not all_matches:
            return {}
        
        # ä¿å­˜å‰10ä¸ªæœ€ä½³åŒ¹é…çš„å¯è§†åŒ–
        best_matches = all_matches[:10]
        
        print(f"\n{Fore.YELLOW}ğŸ’¾ ä¿å­˜æœ€ä½³åŒ¹é…ç»“æœ...")
        
        for i, match in enumerate(best_matches):
            # åˆ›å»ºé«˜è´¨é‡å¯è§†åŒ–
            vis_frame = self.create_detailed_visualization(
                match['frame'], 
                [match], 
                description_features,
                f"åŒ¹é… #{i+1} - åˆ†æ•°: {match['match_analysis']['total_score']:.3f}"
            )
            
            # ä¿å­˜å›¾åƒ
            filename = f"match_{i+1:02d}_frame_{match['frame_number']}_score_{match['match_analysis']['total_score']:.3f}.jpg"
            filepath = os.path.join(output_dir, filename)
            cv2.imwrite(filepath, vis_frame)
            
            print(f"  âœ… ä¿å­˜ {filename}")
        
        # åˆ›å»ºç»¼åˆæŠ¥å‘Š
        report = {
            'search_info': {
                'timestamp': datetime.now().isoformat(),
                'description': description_features['raw_text'],
                'parsed_features': description_features,
                'total_matches': len(all_matches)
            },
            'best_matches': []
        }
        
        for i, match in enumerate(best_matches):
            match_info = {
                'rank': i + 1,
                'frame_number': match['frame_number'],
                'person_id': match['person_id'],
                'bbox': match['bbox'],
                'size': match['size'],
                'confidence': match['confidence'],
                'match_score': match['match_analysis']['total_score'],
                'matched_features': match['match_analysis']['matched_features']
            }
            report['best_matches'].append(match_info)
        
        # ä¿å­˜JSONæŠ¥å‘Š
        with open(os.path.join(output_dir, 'search_report.json'), 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºæ±‡æ€»å¯è§†åŒ–
        self.create_summary_visualization(best_matches, description_features, output_dir)
        
        # æ‰“å°æœ€ä½³ç»“æœ
        print(f"\n{Fore.CYAN}ğŸ† æœ€ä½³åŒ¹é…ç»“æœ:")
        for i, match in enumerate(best_matches[:5]):
            score = match['match_analysis']['total_score']
            print(f"  {i+1}. å¸§{match['frame_number']} - åˆ†æ•°: {score:.3f}")
            if match['match_analysis']['matched_features']['colors']:
                for color, info in match['match_analysis']['matched_features']['colors'].items():
                    print(f"     {color}: {info['ratio']:.1%} åœ¨ {info['region']}")
        
        return report
    
    def create_detailed_visualization(self, frame: np.ndarray, matches: List[Dict], 
                                   description_features: Dict, title: str) -> np.ndarray:
        """åˆ›å»ºè¯¦ç»†çš„å¯è§†åŒ–å›¾åƒ"""
        vis_frame = frame.copy()
        
        for match in matches:
            x1, y1, x2, y2 = match['bbox']
            score = match['match_analysis']['total_score']
            
            # æ ¹æ®åˆ†æ•°é€‰æ‹©é¢œè‰²
            if score > 0.7:
                color = (0, 255, 0)  # ç»¿è‰² - é«˜åŒ¹é…
                thickness = 4
            elif score > 0.4:
                color = (0, 255, 255)  # é»„è‰² - ä¸­ç­‰åŒ¹é…
                thickness = 3
            else:
                color = (0, 100, 255)  # æ©™è‰² - ä½åŒ¹é…
                thickness = 2
            
            # ç»˜åˆ¶è¾¹æ¡†
            cv2.rectangle(vis_frame, (x1, y1), (x2, y2), color, thickness)
            
            # å‡†å¤‡æ ‡ç­¾ä¿¡æ¯
            labels = [
                f"Person {match['person_id']}",
                f"Score: {score:.3f}",
                f"Conf: {match['confidence']:.3f}",
                f"Size: {match['size'][0]}x{match['size'][1]}"
            ]
            
            # æ·»åŠ åŒ¹é…çš„é¢œè‰²ä¿¡æ¯
            if match['match_analysis']['matched_features']['colors']:
                for color_name, info in match['match_analysis']['matched_features']['colors'].items():
                    labels.append(f"{color_name}: {info['ratio']:.1%}")
            
            # ç»˜åˆ¶ä¿¡æ¯èƒŒæ™¯
            label_height = len(labels) * 25 + 10
            label_width = 250
            cv2.rectangle(vis_frame, (x1, y1 - label_height), 
                         (x1 + label_width, y1), color, -1)
            
            # ç»˜åˆ¶æ–‡å­—
            for i, label in enumerate(labels):
                y_pos = y1 - label_height + 20 + i * 25
                cv2.putText(vis_frame, label, (x1 + 5, y_pos),
                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
        
        # å¯é€‰ä¿¡æ¯æ¡ï¼ˆé»˜è®¤å…³é—­ï¼‰
        SHOW_HEADER = False
        if SHOW_HEADER:
            info_bg_height = 120
            cv2.rectangle(vis_frame, (10, 10), (vis_frame.shape[1] - 10, info_bg_height), 
                         (50, 50, 50), -1)
            info_lines = [
                title,
                f"Target: {description_features['raw_text']}",
                f"Colors: {', '.join(description_features['colors'])}",
                f"Clothing: {', '.join(description_features['clothing'])}"
            ]
            for i, line in enumerate(info_lines):
                cv2.putText(vis_frame, line, (20, 40 + i * 25),
                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        return vis_frame
    
    def create_summary_visualization(self, best_matches: List[Dict], 
                                   description_features: Dict, output_dir: str):
        """åˆ›å»ºæ±‡æ€»å¯è§†åŒ–"""
        if not best_matches:
            return
        
        # åˆ›å»ºmatplotlibå›¾å½¢
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'Summary - "{description_features["raw_text"]}"', 
                    fontsize=16, fontweight='bold')
        
        # æ˜¾ç¤ºå‰6ä¸ªæœ€ä½³åŒ¹é…
        for i in range(min(6, len(best_matches))):
            row = i // 3
            col = i % 3
            ax = axes[row, col]
            
            match = best_matches[i]
            frame = match['frame']
            x1, y1, x2, y2 = match['bbox']
            
            # æ˜¾ç¤ºå›¾åƒ
            ax.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            
            # æ·»åŠ è¾¹æ¡†
            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, 
                                   linewidth=3, edgecolor='red', facecolor='none')
            ax.add_patch(rect)
            
            # è®¾ç½®æ ‡é¢˜ï¼ˆè‹±æ–‡ï¼Œé¿å…ä¸­æ–‡å­—ä½“è­¦å‘Šï¼‰
            title = f"#{i+1}: Score {match['match_analysis']['total_score']:.3f}\n"
            title += f"Frame {match['frame_number']}, Person {match['person_id']}"
            
            ax.set_title(title, fontsize=10, fontweight='bold')
            ax.axis('off')
        
        # éšè—å¤šä½™çš„å­å›¾
        for i in range(len(best_matches), 6):
            row = i // 3
            col = i % 3
            axes[row, col].axis('off')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'summary_visualization.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ… ä¿å­˜æ±‡æ€»å¯è§†åŒ–: summary_visualization.png")

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 3:
        print(f"{Fore.CYAN}ç”¨æ³•: python universal_person_finder.py <è§†é¢‘æ–‡ä»¶> <æè¿°æ–‡æœ¬> [æœ€å¤§å¸§æ•°]")
        print(f"{Fore.CYAN}ç¤ºä¾‹:")
        print(f"  python universal_person_finder.py video.mp4 'çº¢è‰²å¤´ç›”çš„äºº' 15")
        print(f"  python universal_person_finder.py video.mp4 'person with blue jacket' 20")
        print(f"  python universal_person_finder.py video.mp4 'ç©¿å·¥è£…çš„å·¥äºº' 10")
        return
    
    video_path = sys.argv[1]
    description = sys.argv[2]
    max_frames = int(sys.argv[3]) if len(sys.argv) > 3 else 15
    
    try:
        finder = UniversalPersonFinder()
        results = finder.find_best_matches(video_path, description, max_frames)
        
    except Exception as e:
        print(f"{Fore.RED}âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 